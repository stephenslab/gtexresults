{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Mashing the GTEx V8 release\n",
    "\n",
    "Here I run the latest `flashr + mashr` pipeline on the latest GTEx release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "*Version: 2018.9.22 by Gao Wang*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table class=\"revision_table\">\n",
       "        <tr>\n",
       "        <th>Revision</th>\n",
       "        <th>Author</th>\n",
       "        <th>Date</th>\n",
       "        <th>Message</th>\n",
       "        <tr>\n",
       "        <tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/ca56229760bd274d193689dbe9bf3efea8103b65/GIT/github/gtexresults/workflows/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">ca56229<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-23</td>\n",
       "<td>Add posterior computation for input data-set</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/7db5500fd4549d39a21360fbcbf689bf5576094f/GIT/github/gtexresults/workflows/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">7db5500<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-23</td>\n",
       "<td>Notes on GTEx V8 data conversion steps</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/68d2571e7d6284d02ac42d53e901968546298251/GIT/github/gtexresults/workflows/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">68d2571<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-05-30</td>\n",
       "<td>Add mashr_flashr workflow</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%revisions -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "`fastqtl` summary statistics data were obtained from dbGaP (data on CRI at UChicago Genetic Medicine). It has 49 tissues. [more description to come]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Some `bash` variables\n",
    "\n",
    "```\n",
    "input_dir=/project/compbio/GTEx_dbGaP/GTEx_Analysis_2017-06-05_v8/eqtl/GTEx_Analysis_v8_eQTL_all_associations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Preparing MASH input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Using an established workflow (which takes 33hrs to run on a cluster system as configured by `data/fe961153.localhost.yml`; see inside `fastqtl_to_mash.ipynb` for a note on computing environment),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "sos run workflows/fastqtl_to_mash.ipynb  -c data/fe961153.localhost.yml --data-list $input_dir/FastQTLSumStats.list --common-suffix \".allpairs.txt\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "I obtained the \"mashable\" data-set in the same format [as described here](https://stephenslab.github.io/gtexresults/gtexdata.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Some data integrity check\n",
    "\n",
    "1. Check if I get the same number of groups (genes) at the end of HDF5 data conversion:\n",
    "\n",
    "```\n",
    "$ zcat Whole_Blood.allpairs.txt.gz | cut -f1 | sort -u | wc -l\n",
    "20316\n",
    "$ h5ls Whole_Blood.allpairs.txt.h5 | wc -l\n",
    "20315\n",
    "```\n",
    "\n",
    "The results agreed on Whole Blood sample (the original data has a header thus one line more than the H5 version). We should be good (since the pipeline reported success for all other files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Data & job summary\n",
    "\n",
    "The command above took 33 hours on UChicago RCC `midway2`. \n",
    "\n",
    "```\n",
    "[MW] cat FastQTLSumStats.log\n",
    "39832 out of 39832 groups merged!\n",
    "```\n",
    "\n",
    "So we have a total of 39832 genes (union of 49 tissues).\n",
    "\n",
    "```\n",
    "[MW] cat FastQTLSumStats.portable.log\n",
    "15636 out of 39832 groups extracted!\n",
    "```\n",
    "\n",
    "We have 15636 groups without missing data in any tissue. This will be used to train the MASH model.\n",
    "\n",
    "The \"mashable\" data file is `FastQTLSumStats.mash.rds`, 124Mb serialized R file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Multivariate adaptive shrinkage (MASH) analysis of eQTL data\n",
    "\n",
    "Below is a \"blackbox\" implementation of the `mashr` eQTL workflow -- blackbox in the sense that you can run this pipeline as an executable, without thinking too much about it, if you see your problem fits our GTEx analysis scheme. However when reading it as a notebook it is a good source of information to help developing your own `mashr` analysis procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Since the submission to biorxiv of Urbut 2017 we have improved implementation of MASH algorithm and made a new R package, [`mashr`](https://github.com/stephenslab/mashr). Major improvements compared to Urbut 2017 are:\n",
    "\n",
    "1. Faster computation of likelihood and posterior matrices via matrix algebra tricks and a C++ implementation.\n",
    "2. Faster computation of MASH mixture via convex optimization.\n",
    "3. Use `FLASH` method, instead of `SFA`, for prior covariance matrices.\n",
    "\n",
    "At this point, the input data have already been converted from the original eQTL summary statistics to a format convenient for analysis in MASH, as a result of running the data conversion pipeline in `fastqtl_to_mash.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./mashr_flashr_workflow_output')\n",
    "# Input summary statistics data\n",
    "parameter: data = path(\"fastqtl_to_mash_output/FastQTLSumStats.mash.rds\")\n",
    "# Use vhat estimated from near NULL data\n",
    "parameter: vhat = 1\n",
    "# Use alpha = 1 for EZ model; 0 for EE model\n",
    "parameter: alpha = 1\n",
    "# Number of components in PCA analysis for prior\n",
    "# default to 3 as in mash paper\n",
    "parameter: npc = 3\n",
    "parameter: mosek_license = file_target(\"~/.mosek.lic\")\n",
    "flash_data = file_target(f\"{cwd:a}/{data:bn}.flash.rds\")\n",
    "fail_if(not mosek_license.is_file(), msg = f'Please put a valid copy (NOT a symbolic link!) of MOSEK license to: \\n``{mosek_license}``')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run mashr_flashr_workflow.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  flash\n",
      "  mash\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd mashr_flashr_workflow_output (as path)\n",
      "  --data data/FastQTLSumStats.mash.rds (as path)\n",
      "                        Input summary statistics data\n",
      "  --vhat 1 (as int)\n",
      "                        Use vhat estimated from near NULL data\n",
      "  --alpha 1 (as int)\n",
      "                        Use alpha = 1 for EZ model; 0 for EE model\n",
      "  --mosek-license /home/gaow/.mosek.lic (as file_target)\n",
      "\n",
      "Sections\n",
      "  flash:                Perform FLASH analysis (time estimate: 20min)\n",
      "  mash_1:               Compute data-driven / canonical prior matrices (time\n",
      "                        estimate: ~2h)\n",
      "    Workflow Options:\n",
      "      --npc 3 (as int)\n",
      "                        number of PC, default to 3 as in mash paper\n",
      "  mash_2:               Fit MASH mixture model (time estimate: <5min)\n"
     ]
    }
   ],
   "source": [
    "!sos run mashr_flashr_workflow.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Fitting the MASH model \n",
    "\n",
    "Main reference are our `mashr` vignettes [this for mashr eQTL outline](https://stephenslab.github.io/mashr/articles/eQTL_outline.html) and [this for using FLASH prior](https://github.com/stephenslab/mashr/blob/master/vignettes/flash_mash.Rmd). \n",
    "The latter was written recently specifically for this effort, and will likely be subject to changes for future versions.\n",
    "\n",
    "```bash\n",
    "sos run workflows/mashr_flashr_workflow.ipynb mash # --data ... --cwd ...\n",
    "```\n",
    "\n",
    "Current implementation requires that you put a copy of [MOSEK license file](https://www.mosek.com/products/academic-licenses) to `<workdir>/mosek.lic` (ie, `mashr_flashr_workflow_output/mosek.lic` if you did not change any settings below). This may also likely change in the future.\n",
    "\n",
    "The outcome of this workflow should be found under `./mashr_flashr_workflow_output` folder (can be configured). File names have pattern `*.mash_model_*.rds`. They can be used to computer posterior for input list of gene-SNP pairs (see next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### `flashr` prior covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Perform FLASH analysis (time estimate: 20min)\n",
    "[flash: provides = flash_data]\n",
    "depends: R_library(\"mashr@stephenslab/flashr\"), R_library('mixSQP@youngseok-kim/mixSQP')\n",
    "input: f\"{data:a}\"\n",
    "output: flash_data\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(flashr)\n",
    "    library(mixSQP)\n",
    "    library(mashr)\n",
    "    \n",
    "    my_init_fn <- function(Y, K = 1) {\n",
    "      ret = flashr:::udv_si(Y, K)\n",
    "      pos_sum = sum(ret$v[ret$v > 0])\n",
    "      neg_sum = -sum(ret$v[ret$v < 0])\n",
    "      if (neg_sum > pos_sum) {\n",
    "        return(list(u = -ret$u, d = ret$d, v = -ret$v))\n",
    "      } else\n",
    "      return(ret)\n",
    "    }\n",
    "\n",
    "    flash_pipeline = function(data, ...) {\n",
    "      ## current state-of-the art\n",
    "      ## suggested by Jason Willwerscheid\n",
    "      ## cf: discussion section of\n",
    "      ## https://willwerscheid.github.io/MASHvFLASH/MASHvFLASHnn2.html\n",
    "      ebnm_fn = \"ebnm_ash\"\n",
    "      ebnm_param = list(l = list(mixcompdist = \"normal\",\n",
    "                               optmethod = \"mixSQP\"),\n",
    "                        f = list(mixcompdist = \"+uniform\",\n",
    "                               optmethod = \"mixSQP\"))\n",
    "      ##\n",
    "      fl_g <- flashr:::flash_greedy_workhorse(data,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    init_fn = \"my_init_fn\",\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      fl_b <- flashr:::flash_backfit_workhorse(data,\n",
    "                    f_init = fl_g,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      return(fl_b)\n",
    "    }\n",
    "\n",
    "    cov_flash = function(data, subset = NULL, non_canonical = FALSE, save_model = NULL) {\n",
    "      if(is.null(subset)) subset = 1:mashr:::n_effects(data)\n",
    "      b.center = apply(data$Bhat, 2, function(x) x - mean(x))\n",
    "      ## Only keep factors with at least two values greater than 1 / sqrt(n)\n",
    "      find_nonunique_effects <- function(fl) {\n",
    "        thresh <- 1/sqrt(ncol(fl$fitted_values))\n",
    "        vals_above_avg <- colSums(fl$ldf$f > thresh)\n",
    "        nonuniq_effects <- which(vals_above_avg > 1)\n",
    "        return(fl$ldf$f[, nonuniq_effects, drop = FALSE])\n",
    "      }\n",
    "\n",
    "      fmodel = flash_pipeline(b.center)\n",
    "      if (non_canonical)\n",
    "          flash_f = find_nonunique_effects(fmodel)\n",
    "      else \n",
    "          flash_f = fmodel$ldf$f\n",
    "      ## row.names(flash_f) = colnames(b)\n",
    "      if (!is.null(save_model)) saveRDS(list(model=fmodel, factors=flash_f), save_model)\n",
    "      U.flash = c(cov_from_factors(t(as.matrix(flash_f)), \"FLASH\"),\n",
    "          list(\"tFLASH\" = t(fmodel$fitted_values) %*% fmodel$fitted_values / nrow(fmodel$fitted_values)))\n",
    "      return(U.flash)\n",
    "    }\n",
    "    ##\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(as.matrix(dat$strong.b), as.matrix(dat$strong.s))\n",
    "    res = cov_flash(dat, non_canonical = TRUE, save_model = \"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### `mashr` multivariate eQTL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Compute data-driven / canonical prior matrices (time estimate: 2h ~ 12h for ~30 49 by 49 matrix mixture)\n",
    "[mash_1]\n",
    "depends: R_library(\"ExtremeDeconvolution\"), R_library(\"mashr@stephenslab/mashr\"), flash_data\n",
    "input: f\"{data:a}\"\n",
    "output: f\"{cwd:a}/{data:bn}.FL_PC{npc}.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    library(mashr)\n",
    "    dat = readRDS(${data:ar})\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${alpha})\n",
    "    # FLASH matrices\n",
    "    U.flash = readRDS(${flash_data:r})\n",
    "    # SVD matrices\n",
    "    U.pca = cov_pca(mash_data, ${npc})\n",
    "    # Emperical cov matrix\n",
    "    z = dat$strong.b / dat$strong.s\n",
    "    z[which(is.nan(z))] = 0\n",
    "    U.cov = apply(z, 2, function(x) x - mean(x))\n",
    "    # Denoised data-driven matrices\n",
    "    U.ed = cov_ed(mash_data, c(U.flash, U.pca, list(\"XX\" = t(U.cov) %*% U.cov / nrow(U.cov))), logfile=${_output:nr})\n",
    "    # Canonical matrices\n",
    "    U.can = cov_canonical(mash_data)\n",
    "    saveRDS(c(U.ed, U.can), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Fit MASH mixture model (time estimate: <15min for 70K by 49 matrix)\n",
    "[mash_2]\n",
    "depends: R_library(\"REBayes\")\n",
    "output: f\"{_input:n}.{'mash_model_est_v' if vhat else 'mash_model_diag_v'}.rds\", f\"{_input:nn}.Vhat.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd, env = {'MOSEKLM_LICENSE_FILE': str(mosek_license)}\n",
    "    library(mashr)\n",
    "    dat = readRDS(${data:ar})\n",
    "    vhat = estimate_null_correlation(mash_set_data(dat$random.b, Shat=dat$random.s))\n",
    "    mash_data = mash_set_data(dat$random.b, Shat=dat$random.s, alpha=${alpha}, V=${\"vhat\" if vhat else \"diag(nrow(vhat))\"})\n",
    "    saveRDS(mash(mash_data, Ulist = readRDS(${_input:r}), outputlevel = 1), ${_output[0]:r})\n",
    "    saveRDS(vhat, ${_output[1]:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Computing MASH posteriors\n",
    "\n",
    "In the GTEx V6 paper we assumed one eQTL per gene and applied the model learned above to those SNPs. Under that assumption, the input data for posterior calculation will be the `dat$strong.*` matrices.\n",
    "It is a fairly straightforward procedure as shown in [this vignette](https://stephenslab.github.io/mashr/articles/eQTL_outline.html).\n",
    "\n",
    "But it is often more interesting to apply MASH to given list of eQTLs, eg, from those from fine-mapping results. In GTEx V8 analysis we obtain such gene-SNP pairs from DAP-G fine-mapping analysis. See [this notebook](https://gaow.github.io/mash-gtex-v8/analysis/Independent_eQTL_Results.html) for how the input data is prepared. The workflow below takes a number of input chunks (each chunk is a list of matrices `dat$Bhat` and `dat$Shat`) \n",
    "and computes posterior for each chunk. It is therefore suited for running in parallel posterior computation for all gene-SNP pairs, if input data chunks are provided.\n",
    "\n",
    "**Note: using `task:` option in the workflow below, I submit the jobs to UChicago RCC cluster. Non-UChicago users can comment out `task:` line, or configure it to use your computational system.**\n",
    "\n",
    "```\n",
    "data_dir=/project/compbio/GTEx_eQTL/independent_eQTL\n",
    "sos run workflows/mashr_flashr_workflow.ipynb posterior \\\n",
    "    -c data/fe961153.localhost.yml \\\n",
    "    --posterior-input $data_dir/DAPG_pip_gt_0.01-AllTissues/DAPG_pip_gt_0.01-AllTissues.*.rds \\\n",
    "                      $data_dir/ConditionalAnalysis_AllTissues/ConditionalAnalysis_AllTissues.*.rds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Apply posterior calculations\n",
    "[posterior]\n",
    "parameter: mash_model = path(f\"{cwd:a}/{data:bn}.FL_PC{npc}.{'mash_model_est_v' if vhat else 'mash_model_diag_v'}.rds\")\n",
    "parameter: posterior_input = paths()\n",
    "vhat_file = path(f\"{cwd:a}/{data:bn}.Vhat.rds\")\n",
    "stop_if(len(posterior_input) == 0)\n",
    "for p in posterior_input:\n",
    "    fail_if(not p.is_file(), msg = f'Cannot find posterior input file ``{p}``')\n",
    "input: posterior_input, group_by = 1, concurrent = True\n",
    "output: f\"{_input:n}.posterior.rds\"\n",
    "task: trunk_workers = 1, queue = 'midway2', walltime = '2h', trunk_size = 1, mem = '20G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    library(mashr)\n",
    "    strong = readRDS(${_input:r})\n",
    "    vhat = readRDS(${vhat_file:r})\n",
    "    mash_data = mash_set_data(strong$Bhat, Shat=strong$Shat, alpha=${alpha}, V=${\"vhat\" if vhat else \"diag(nrow(vhat))\"})\n",
    "    saveRDS(mash_compute_posterior_matrices(readRDS(${mash_model:r}), mash_data), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. The outcome of the `[posterior]` step should produce a number of serialized R objects `*.batch_*.posterior.rds` (can be loaded to R via `readRDS()`) -- I chopped data to batches to take advantage of computing in multiple cluster nodes. It should be self-explanary but please let me know otherwise.\n",
    "2. Other posterior related files are:\n",
    "    1. `*.batch_*.yaml`: gene-SNP pairs of interest, identified elsewhere (eg. fine-mapping analysis). \n",
    "    2. The corresponding univariate analysis summary statistics from `*.batch_*.yaml` are extracted and saved to `*.batch_*.rds`, as input to the `[posterior]` step.\n",
    "    3. Note the `*.batch_*.stdout` file documents some SNPs found in fine-mapping results but not found in the original `fastqtl` output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.16.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

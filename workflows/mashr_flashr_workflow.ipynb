{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Mashing the GTEx V8 release\n",
    "\n",
    "Here I run the latest `flashr + mashr` pipeline on the latest GTEx release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "*Version: 2019.01.24 by Gao Wang and Yuxin Zou*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table class=\"revision_table\">\n",
       "        <tr>\n",
       "        <th>Revision</th>\n",
       "        <th>Author</th>\n",
       "        <th>Date</th>\n",
       "        <th>Message</th>\n",
       "        <tr>\n",
       "        <tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/80e89a86f9ce380b7f30dc4dd64ef4ec632ef2d0/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">80e89a8<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2019-01-28</td>\n",
       "<td>Add a note on HPC job submission</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/982a1b4cc4d8a14a77587fd55a825bf9ef00391e/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">982a1b4<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2019-01-28</td>\n",
       "<td>Implement new $\\hat{V}$ estimation method (with Yuxin Zou)</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/515e86951d09a0f2c1d4d3efde43882bfb75427e/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">515e869<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-22</td>\n",
       "<td>Add a prompt for empty input posterior</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/ba9b20e5d705b20cda80826989293022452c3101/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">ba9b20e<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-22</td>\n",
       "<td>Add posterior calculation for input 'strong' set</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/49494706a413999201362e72c9e20cbb95351be2/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">4949470<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-22</td>\n",
       "<td>Fix mashr null correlation estimate interface</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/6145b3366b7850eac2e10bdd69634f6482e3359f/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">6145b33<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-21</td>\n",
       "<td>Add --optmethod to configure convex optimization method to use</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/485381c8483bc4b770f8648b325a6386cd1a68d9/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">485381c<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-20</td>\n",
       "<td>Fix mixSQP package name #2</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/c587791ce58a5d393782466ba762a32544edcf0e/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">c587791<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-20</td>\n",
       "<td>Use the first cran release of mixSQP in default flashr workflow</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/092e20680cce52f594dd7fb95ee2c6b8ea85f036/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">092e206<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-24</td>\n",
       "<td>Minor edits to documentation</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/4a5f2b78f94619f5759448e1fd0a9ba8fb600cb0/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">4a5f2b7<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-24</td>\n",
       "<td>Add notes to results</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/e653d39708c6ba5b96da27472aed2896232814a8/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">e653d39<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-24</td>\n",
       "<td>Configure posterior computation resources</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/ca56229760bd274d193689dbe9bf3efea8103b65/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">ca56229<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-23</td>\n",
       "<td>Add posterior computation for input data-set</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/7db5500fd4549d39a21360fbcbf689bf5576094f/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">7db5500<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-23</td>\n",
       "<td>Notes on GTEx V8 data conversion steps</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/68d2571e7d6284d02ac42d53e901968546298251/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">68d2571<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-05-30</td>\n",
       "<td>Add mashr_flashr workflow</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%revisions -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "`fastqtl` summary statistics data were obtained from dbGaP (data on CRI at UChicago Genetic Medicine). It has 49 tissues. [more description to come]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Preparing MASH input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Using an established workflow (which takes 33hrs to run on a cluster system as configured by `midway2.yml`; see inside `fastqtl_to_mash.ipynb` for a note on computing environment),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "INPUT_DIR=/project/compbio/GTEx_dbGaP/GTEx_Analysis_2017-06-05_v8/eqtl/GTEx_Analysis_v8_eQTL_all_associations\n",
    "JOB_OPT=\"-c midway2.yml -q midway2\"\n",
    "sos run workflows/fastqtl_to_mash.ipynb --data-list $INPUT_DIR/FastQTLSumStats.list --common-suffix \".allpairs.txt\" $JOB_OPT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "As a result of command above I obtained the \"mashable\" data-set in the same format [as described here](https://stephenslab.github.io/gtexresults/gtexdata.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Some data integrity check\n",
    "\n",
    "1. Check if I get the same number of groups (genes) at the end of HDF5 data conversion:\n",
    "\n",
    "```\n",
    "$ zcat Whole_Blood.allpairs.txt.gz | cut -f1 | sort -u | wc -l\n",
    "20316\n",
    "$ h5ls Whole_Blood.allpairs.txt.h5 | wc -l\n",
    "20315\n",
    "```\n",
    "\n",
    "The results agreed on Whole Blood sample (the original data has a header thus one line more than the H5 version). We should be good (since the pipeline reported success for all other files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Data & job summary\n",
    "\n",
    "The command above took 33 hours on UChicago RCC `midway2`. \n",
    "\n",
    "```\n",
    "[MW] cat FastQTLSumStats.log\n",
    "39832 out of 39832 groups merged!\n",
    "```\n",
    "\n",
    "So we have a total of 39832 genes (union of 49 tissues).\n",
    "\n",
    "```\n",
    "[MW] cat FastQTLSumStats.portable.log\n",
    "15636 out of 39832 groups extracted!\n",
    "```\n",
    "\n",
    "We have 15636 groups without missing data in any tissue. This will be used to train the MASH model.\n",
    "\n",
    "The \"mashable\" data file is `FastQTLSumStats.mash.rds`, 124Mb serialized R file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Multivariate adaptive shrinkage (MASH) analysis of eQTL data\n",
    "\n",
    "Below is a \"blackbox\" implementation of the `mashr` eQTL workflow -- blackbox in the sense that you can run this pipeline as an executable, without thinking too much about it, if you see your problem fits our GTEx analysis scheme. However when reading it as a notebook it is a good source of information to help developing your own `mashr` analysis procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Since the submission to biorxiv of Urbut 2017 we have improved implementation of MASH algorithm and made a new R package, [`mashr`](https://github.com/stephenslab/mashr). Major improvements compared to Urbut 2017 are:\n",
    "\n",
    "1. Faster computation of likelihood and posterior quantities via matrix algebra tricks and a C++ implementation.\n",
    "2. Faster computation of MASH mixture via convex optimization.\n",
    "3. Replace `SFA` with `FLASH`, a new sparse factor analysis method to generate prior covariance candidates.\n",
    "4. Improve estimate of residual variance $\\hat{V}$.\n",
    "\n",
    "At this point, the input data have already been converted from the original eQTL summary statistics to a format convenient for analysis in MASH, as a result of running the data conversion pipeline in `fastqtl_to_mash.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Example command:\n",
    "\n",
    "\n",
    "```bash\n",
    "JOB_OPT=\"-j 8\"\n",
    "#JOB_OPT=\"-c midway2.yml -q midway2\"\n",
    "sos run workflows/mashr_flashr_workflow.ipynb mash $JOB_OPT # --data ... --cwd ... --vhat ...\n",
    "```\n",
    "\n",
    "**FIXME: add comments on submitting jobs to HPC. Here we use the UChicago RCC cluster but other users can similarly configure their computating system to run the pipeline on HPC.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Global parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./mashr_flashr_workflow_output')\n",
    "# Input summary statistics data\n",
    "parameter: data = path(\"fastqtl_to_mash_output/FastQTLSumStats.mash.rds\")\n",
    "# Prefix of output files. If not specified, it will derive it from data.\n",
    "# If it is specified, for example, `--output-prefix AnalysisResults`\n",
    "# It will save output files as `{cwd}/AnalysisResults*`.\n",
    "parameter: output_prefix = ''\n",
    "# Exchangable effect (EE) or exchangable z-scores (EZ)\n",
    "parameter: effect_model = 'EZ'\n",
    "# Identifier of $\\hat{V}$ estimate file\n",
    "# It should be available as {cwd}/{data:bn}.V_{vhat}.rds\n",
    "parameter: vhat = 'simple'\n",
    "# Additional label for vhat file\n",
    "parameter: vhat_file_label = ''\n",
    "# default method for convex optimization\n",
    "parameter: optmethod = \"mixSQP\"\n",
    "# Path to mosek license file, if `--optmethod mixIP` is used\n",
    "parameter: mosek_license = file_target(\"~/.mosek.lic\")\n",
    "# Most established heavy-lifting computations are in C++ via Rcpp\n",
    "# but some experimental features are in R. This argument allows one\n",
    "# to switch between implementations\n",
    "parameter: implementation = 'Rcpp'\n",
    "# Number of components in PCA analysis for prior\n",
    "# default to 3 as in mash paper\n",
    "# set it to 0 to not use PCA priors\n",
    "parameter: npc = 3\n",
    "data = data.absolute()\n",
    "if len(output_prefix) == 0:\n",
    "    output_prefix = f\"{data:bn}\"\n",
    "if vhat_file_label and not vhat_file_label.startswith('_'):\n",
    "    vhat_file_label = '_' + vhat_file_label\n",
    "flash_data = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.flash.rds\")\n",
    "prior_data = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.FL_PC{npc}.rds\")\n",
    "vhat_data = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.FL_PC{npc}.V_{vhat}{vhat_file_label}.rds\")\n",
    "fail_if(optmethod == \"mixIP\" and not mosek_license.is_file(), msg = f'To use mixIP optimization, please put a valid copy (NOT a symbolic link!) of MOSEK license to: \\n``{mosek_license}``')\n",
    "\n",
    "def sort_uniq(seq):\n",
    "    seen = set()\n",
    "    return [x for x in seq if not (x in seen or seen.add(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run mashr_flashr_workflow.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  flash\n",
      "  prior\n",
      "  vhat_identity\n",
      "  vhat_simple\n",
      "  vhat_mle\n",
      "  vhat_corshrink_xcondition\n",
      "  vhat_simple_specific\n",
      "  mash\n",
      "  posterior\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd mashr_flashr_workflow_output (as path)\n",
      "  --data fastqtl_to_mash_output/FastQTLSumStats.mash.rds (as path)\n",
      "                        Input summary statistics data\n",
      "  --output-prefix ''\n",
      "                        Prefix of output files. If not specified, it will derive\n",
      "                        it from data. If it is specified, for example,\n",
      "                        `--output-prefix AnalysisResults` It will save output\n",
      "                        files as `{cwd}/AnalysisResults*`.\n",
      "  --effect-model EZ\n",
      "                        Exchangable effect (EE) or exchangable z-scores (EZ)\n",
      "  --vhat simple\n",
      "                        Identifier of $\\hat{V}$ estimate file It should be\n",
      "                        available as {cwd}/{data:bn}.V_{vhat}.rds\n",
      "  --vhat-file-label ''\n",
      "                        Additional label for vhat file\n",
      "  --optmethod mixSQP\n",
      "                        default method for convex optimization\n",
      "  --mosek-license /home/gaow/.mosek.lic (as file_target)\n",
      "                        Path to mosek license file, if `--optmethod mixIP` is\n",
      "                        used\n",
      "  --implementation Rcpp\n",
      "                        Most established heavy-lifting computations are in C++\n",
      "                        via Rcpp but some experimental features are in R. This\n",
      "                        argument allows one to switch between implementations\n",
      "  --npc 3 (as int)\n",
      "                        Number of components in PCA analysis for prior default\n",
      "                        to 3 as in mash paper set it to 0 to not use PCA priors\n",
      "\n",
      "Sections\n",
      "  flash:                Perform FLASH analysis (time estimate: 20min)\n",
      "  prior:                Compute data-driven / canonical prior matrices (time\n",
      "                        estimate: 2h ~ 12h for ~30 49 by 49 matrix mixture)\n",
      "  vhat_identity:        V estimate: \"identity\" method\n",
      "  vhat_simple:          V estimate: \"simple\" method (using null z-scores)\n",
      "  vhat_mle:             V estimate: \"mle\" method\n",
      "    Workflow Options:\n",
      "      --n-subset 6000 (as int)\n",
      "                        number of samples to use\n",
      "      --max-iter 6 (as int)\n",
      "                        maximum number of iterations\n",
      "  vhat_corshrink_xcondition_1: Estimate each V separately via corshrink\n",
      "    Workflow Options:\n",
      "      --util-script /project/mstephens/gtex/scripts/SumstatQuery.R (as path)\n",
      "                        Utility script\n",
      "      --gene-list . (as path)\n",
      "                        List of genes to analyze\n",
      "  vhat_corshrink_xcondition_2, vhat_simple_specific_2: Consolidate Vhat into one\n",
      "                        file\n",
      "    Workflow Options:\n",
      "      --gene-list . (as path)\n",
      "                        List of genes to analyze\n",
      "  vhat_simple_specific_1: Estimate each V separately via \"simple\" method\n",
      "    Workflow Options:\n",
      "      --util-script /project/mstephens/gtex/scripts/SumstatQuery.R (as path)\n",
      "                        Utility script\n",
      "      --gene-list . (as path)\n",
      "                        List of genes to analyze\n",
      "  mash_1:               Fit MASH mixture model (time estimate: <15min for 70K by\n",
      "                        49 matrix)\n",
      "  mash_2:               Compute posterior for the \"strong\" set of data as in\n",
      "                        Urbut et al 2017. This is optional because most of the\n",
      "                        time we want to apply the MASH model learned on much\n",
      "                        larger data-set.\n",
      "    Workflow Options:\n",
      "      --[no-]compute-posterior (default to True)\n",
      "                        default to True; use --no-compute-posterior to disable\n",
      "                        this\n",
      "      --posterior-vhat-file . (as path)\n",
      "                        input Vhat file for the batch of posterior data\n",
      "  posterior:            Apply posterior calculations\n",
      "    Workflow Options:\n",
      "      --mash-model  path(f\"{vhat_data:n}.mash_model.rds\")\n",
      "\n",
      "      --posterior-input  paths()\n",
      "\n",
      "      --posterior-vhat-files  paths()\n",
      "\n",
      "      --data-table-name ''\n",
      "                        eg, if data is saved in R list as data$strong, then when\n",
      "                        you specify `--data-table-name strong` it will read the\n",
      "                        data as readRDS('{_input:r}')$strong\n"
     ]
    }
   ],
   "source": [
    "sos run mashr_flashr_workflow.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Compute MASH priors \n",
    "\n",
    "Main reference are our `mashr` vignettes [this for mashr eQTL outline](https://stephenslab.github.io/mashr/articles/eQTL_outline.html) and [this for using FLASH prior](https://github.com/stephenslab/mashr/blob/master/vignettes/flash_mash.Rmd). \n",
    "The latter was written recently specifically for this effort, and will likely be subject to changes for future versions.\n",
    "\n",
    "If you use `--optmethod mixIP` you will have to put a copy of [MOSEK license file](https://www.mosek.com/products/academic-licenses) to `<workdir>/mosek.lic` (ie, `mashr_flashr_workflow_output/mosek.lic` if you did not change any settings below). Current default method is `mixSQP`.\n",
    "\n",
    "The outcome of this workflow should be found under `./mashr_flashr_workflow_output` folder (can be configured). File names have pattern `*.mash_model_*.rds`. They can be used to computer posterior for input list of gene-SNP pairs (see next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### `flashr` prior covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Perform FLASH analysis (time estimate: 20min)\n",
    "[flash]\n",
    "depends: R_library(\"flashr\")\n",
    "input: data\n",
    "output: flash_data\n",
    "\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, env = {'MOSEKLM_LICENSE_FILE': str(mosek_license)}, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(flashr)\n",
    "    library(mixsqp)\n",
    "    library(mashr)\n",
    "    \n",
    "    my_init_fn <- function(Y, K = 1) {\n",
    "      ret = flashr:::udv_si(Y, K)\n",
    "      pos_sum = sum(ret$v[ret$v > 0])\n",
    "      neg_sum = -sum(ret$v[ret$v < 0])\n",
    "      if (neg_sum > pos_sum) {\n",
    "        return(list(u = -ret$u, d = ret$d, v = -ret$v))\n",
    "      } else\n",
    "      return(ret)\n",
    "    }\n",
    "\n",
    "    flash_pipeline = function(data, ...) {\n",
    "      ## current state-of-the art\n",
    "      ## suggested by Jason Willwerscheid\n",
    "      ## cf: discussion section of\n",
    "      ## https://willwerscheid.github.io/MASHvFLASH/MASHvFLASHnn2.html\n",
    "      ebnm_fn = \"ebnm_ash\"\n",
    "      ebnm_param = list(l = list(mixcompdist = \"normal\",\n",
    "                               optmethod = \"${optmethod}\"),\n",
    "                        f = list(mixcompdist = \"+uniform\",\n",
    "                               optmethod = \"${optmethod}\"))\n",
    "      ##\n",
    "      fl_g <- flashr:::flash_greedy_workhorse(data,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    init_fn = \"my_init_fn\",\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      fl_b <- flashr:::flash_backfit_workhorse(data,\n",
    "                    f_init = fl_g,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      return(fl_b)\n",
    "    }\n",
    "\n",
    "    cov_flash = function(data, subset = NULL, non_canonical = FALSE, save_model = NULL) {\n",
    "      if(is.null(subset)) subset = 1:mashr:::n_effects(data)\n",
    "      b.center = apply(data$Bhat, 2, function(x) x - mean(x))\n",
    "      ## Only keep factors with at least two values greater than 1 / sqrt(n)\n",
    "      find_nonunique_effects <- function(fl) {\n",
    "        thresh <- 1/sqrt(ncol(fl$fitted_values))\n",
    "        vals_above_avg <- colSums(fl$ldf$f > thresh)\n",
    "        nonuniq_effects <- which(vals_above_avg > 1)\n",
    "        return(fl$ldf$f[, nonuniq_effects, drop = FALSE])\n",
    "      }\n",
    "\n",
    "      fmodel = flash_pipeline(b.center)\n",
    "      if (non_canonical)\n",
    "          flash_f = find_nonunique_effects(fmodel)\n",
    "      else \n",
    "          flash_f = fmodel$ldf$f\n",
    "      ## row.names(flash_f) = colnames(b)\n",
    "      if (!is.null(save_model)) saveRDS(list(model=fmodel, factors=flash_f), save_model)\n",
    "      if(ncol(flash_f) == 0){\n",
    "        U.flash = list(\"tFLASH\" = t(fmodel$fitted_values) %*% fmodel$fitted_values / nrow(fmodel$fitted_values))\n",
    "      } else{\n",
    "        U.flash = c(cov_from_factors(t(as.matrix(flash_f)), \"FLASH\"),\n",
    "                    list(\"tFLASH\" = t(fmodel$fitted_values) %*% fmodel$fitted_values / nrow(fmodel$fitted_values)))\n",
    "      }\n",
    "      return(U.flash)\n",
    "    }\n",
    "    ##\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mash_set_data(dat$strong.b, dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0})\n",
    "    res = cov_flash(dat, non_canonical = TRUE, save_model = \"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Other priors and refinement via Extreme Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Compute data-driven / canonical prior matrices (time estimate: 2h ~ 12h for ~30 49 by 49 matrix mixture)\n",
    "[prior]\n",
    "depends: R_library(\"mashr\")\n",
    "input: data, output_from('flash')\n",
    "output: prior_data\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0})\n",
    "    # FLASH matrices\n",
    "    U.flash = readRDS(${_input[1]:r})\n",
    "    # SVD matrices\n",
    "    U.pca = ${\"cov_pca(mash_data, %s)\" % npc if npc > 0 else \"list()\"}\n",
    "    # Emperical cov matrix\n",
    "    X.center = apply(mash_data$Bhat, 2, function(x) x - mean(x))\n",
    "    # Denoised data-driven matrices\n",
    "    U.ed = cov_ed(mash_data, c(U.flash, U.pca, list(\"XX\" = t(X.center) %*% X.center / nrow(X.center))), logfile=${_output:nr})\n",
    "    # Canonical matrices\n",
    "    U.can = cov_canonical(mash_data)\n",
    "    saveRDS(c(U.ed, U.can), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Estimate residual variance\n",
    "\n",
    "FIXME: add some narratives here explaining what we do in each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"identity\" method\n",
    "[vhat_identity]\n",
    "input: data\n",
    "output: vhat_data\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    dat = readRDS(${_input:r})\n",
    "    saveRDS(diag(ncol(dat$random.b)), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"simple\" method (using null z-scores)\n",
    "[vhat_simple]\n",
    "depends: R_library(\"mashr\")\n",
    "input: data\n",
    "output: vhat_data\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input:r})\n",
    "    vhat = estimate_null_correlation_simple(mash_set_data(dat$random.b, Shat=dat$random.s))\n",
    "    saveRDS(vhat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"mle\" method\n",
    "[vhat_mle]\n",
    "# number of samples to use\n",
    "parameter: n_subset = 6000\n",
    "# maximum number of iterations\n",
    "parameter: max_iter = 6\n",
    "depends: R_library(\"mashr\")\n",
    "input: data, output_from('prior')\n",
    "output: vhat_data\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    # choose random subset\n",
    "    set.seed(1)\n",
    "    random.subset = sample(1:nrow(dat$random.b), min(${n_subset}, nrow(dat$random.b)))\n",
    "    random.subset = mash_set_data(dat$random.b[random.subset,], dat$random.s[random.subset,], alpha=${1 if effect_model == 'EZ' else 0})\n",
    "    # estimate V mle\n",
    "    vhat = estimate_null_correlation(random.subset, readRDS(${_input[1]:r}), max_iter = ${max_iter})\n",
    "    saveRDS(vhat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Estimate each V separately via corshrink\n",
    "[vhat_corshrink_xcondition_1]\n",
    "# Utility script\n",
    "parameter: util_script = path('/project/mstephens/gtex/scripts/SumstatQuery.R')\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "fail_if(not util_script.is_file() and len(str(util_script)), msg = 'Please specify valid path for --util-script')\n",
    "genes = sort_uniq([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "\n",
    "depends: R_library(\"CorShrink\")\n",
    "input: data, for_each = 'genes'\n",
    "output: f'{vhat_data:n}/{_genes}.rds'\n",
    "\n",
    "task: trunk_workers = 1, walltime = '3m', trunk_size = 500, mem = '3G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    source(${util_script:r})\n",
    "    CorShrink_sum = function(gene, database, z_thresh = 2){\n",
    "      print(gene)\n",
    "      dat <- GetSS(gene, database)\n",
    "      z = dat$\"z-score\"\n",
    "      max_absz = apply(abs(z), 1, max)\n",
    "      nullish = which(max_absz < z_thresh)\n",
    "      # if (length(nullish) < ncol(z)) {\n",
    "        # stop(\"not enough null data to estimate null correlation\")\n",
    "      # }\n",
    "      if (length(nullish) <= 1){\n",
    "        mat = diag(ncol(z))\n",
    "      } else {\n",
    "        nullish_z = z[nullish, ]  \n",
    "        mat = as.matrix(CorShrink::CorShrinkData(nullish_z, ash.control = list(mixcompdist = \"halfuniform\"))$cor)\n",
    "      }\n",
    "      return(mat)\n",
    "    }\n",
    "    V = Corshrink_sum(\"${_genes}\", ${data:r})\n",
    "    saveRDS(V, ${_output:r})\n",
    "\n",
    "# Consolidate Vhat into one file\n",
    "[vhat_corshrink_xcondition_2, vhat_simple_specific_2]\n",
    "depends: R_library(\"parallel\")\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "genes = paths([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "\n",
    "input: group_by = 'all'\n",
    "output: vhat_data\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(parallel)\n",
    "    files = sapply(c(${genes:r,}), function(g) paste0(c(${_input[0]:adr}), '/', g, '.rds'), USE.NAMES=FALSE)\n",
    "    V = mclapply(files, function(i){ readRDS(i) }, mc.cores = 1)\n",
    "    R = dim(V[[1]])[1]\n",
    "    L = length(V)\n",
    "    V.array = array(as.numeric(unlist(V)), dim=c(R, R, L))\n",
    "    saveRDS(V.array, ${_output:ar})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Estimate each V separately via \"simple\" method\n",
    "[vhat_simple_specific_1]\n",
    "# Utility script\n",
    "parameter: util_script = path('/project/mstephens/gtex/scripts/SumstatQuery.R')\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "fail_if(not util_script.is_file() and len(str(util_script)), msg = 'Please specify valid path for --util-script')\n",
    "genes = sort_uniq([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "depends: R_library(\"Matrix\")\n",
    "input: data, for_each = 'genes'\n",
    "output: f'{vhat_data:n}/{_genes}.rds'\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1m', trunk_size = 500, mem = '3G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    source(${util_script:r})\n",
    "    simple_V = function(gene, database, z_thresh = 2){\n",
    "      print(gene)\n",
    "      dat <- GetSS(gene, database)\n",
    "      z = dat$\"z-score\"\n",
    "      max_absz = apply(abs(z), 1, max)\n",
    "      nullish = which(max_absz < z_thresh)\n",
    "      # if (length(nullish) < ncol(z)) {\n",
    "        # stop(\"not enough null data to estimate null correlation\")\n",
    "      # }\n",
    "      if (length(nullish) <= 1){\n",
    "        mat = diag(ncol(z))\n",
    "      } else {\n",
    "        nullish_z = z[nullish, ]\n",
    "        mat = as.matrix(Matrix::nearPD(as.matrix(cov(nullish_z)), conv.tol=1e-06, doSym = TRUE, corr=TRUE)$mat)\n",
    "      }\n",
    "      return(mat)\n",
    "    }\n",
    "    V = simple_V(\"${_genes}\", ${data:r})\n",
    "    saveRDS(V, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### `mashr` mixture model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Fit MASH mixture model (time estimate: <15min for 70K by 49 matrix)\n",
    "[mash_1]\n",
    "depends: R_library(\"mashr\")\n",
    "input: data, output_from(\"prior\"), output_from(f\"vhat_{vhat}\")\n",
    "output: f\"{_input[-1]:n}.mash_model.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, env = {'MOSEKLM_LICENSE_FILE': str(mosek_license)}, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    vhat = readRDS(${_input[2]:r})\n",
    "    mash_data = mash_set_data(dat$random.b, Shat=dat$random.s, alpha=${1 if effect_model == 'EZ' else 0}, V=vhat)\n",
    "    saveRDS(mash(mash_data, Ulist = readRDS(${_input[1]:r}), optmethod = \"${optmethod}\", outputlevel = 1, algorithm.version = \"${implementation}\"), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Optional posterior computations\n",
    "\n",
    "Additionally provide posterior for the \"strong\" set in MASH input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Compute posterior for the \"strong\" set of data as in Urbut et al 2017.\n",
    "# This is optional because most of the time we want to apply the \n",
    "# MASH model learned on much larger data-set.\n",
    "[mash_2]\n",
    "# default to True; use --no-compute-posterior to disable this\n",
    "parameter: compute_posterior = True\n",
    "# input Vhat file for the batch of posterior data\n",
    "parameter: posterior_vhat_file = path()\n",
    "skip_if(not compute_posterior)\n",
    "fail_if(not posterior_vhat_file.is_file() and len(str(posterior_vhat_file)) > 1, msg = 'Cannot find specified --posterior-vhat-file. Please provide the correct path for it!')\n",
    "depends: R_library(\"mashr\")\n",
    "input: data, posterior_vhat_file if posterior_vhat_file.is_file() else output_from(f\"vhat_{vhat}\"), output_from(-1)\n",
    "output: f'{posterior_vhat_file:n}.posterior.rds' if posterior_vhat_file.is_file() else f\"{_input[-1]:nn}.posterior.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    vhat = readRDS(${_input[1]:r})\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, V=vhat)\n",
    "    mash_model = readRDS(${_input[2]:ar})\n",
    "    saveRDS(mash_compute_posterior_matrices(mash_model, mash_data, algorithm.version = \"${implementation}\"), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Compute MASH posteriors\n",
    "\n",
    "In the GTEx V6 paper we assumed one eQTL per gene and applied the model learned above to those SNPs. Under that assumption, the input data for posterior calculation will be the `dat$strong.*` matrices.\n",
    "It is a fairly straightforward procedure as shown in [this vignette](https://stephenslab.github.io/mashr/articles/eQTL_outline.html).\n",
    "\n",
    "But it is often more interesting to apply MASH to given list of eQTLs, eg, from those from fine-mapping results. In GTEx V8 analysis we obtain such gene-SNP pairs from DAP-G fine-mapping analysis. See [this notebook](https://gaow.github.io/mnm-gtex-v8/analysis/Independent_eQTL_Results.html) for how the input data is prepared. The workflow below takes a number of input chunks (each chunk is a list of matrices `dat$Bhat` and `dat$Shat`) \n",
    "and computes posterior for each chunk. It is therefore suited for running in parallel posterior computation for all gene-SNP pairs, if input data chunks are provided.\n",
    "\n",
    "\n",
    "```\n",
    "JOB_OPT=\"-c midway2.yml -q midway2\"\n",
    "DATA_DIR=/project/compbio/GTEx_eQTL/independent_eQTL\n",
    "sos run workflows/mashr_flashr_workflow.ipynb posterior \\\n",
    "    $JOB_OPT \\\n",
    "    --posterior-input $DATA_DIR/DAPG_pip_gt_0.01-AllTissues/DAPG_pip_gt_0.01-AllTissues.*.rds \\\n",
    "                      $DATA_DIR/ConditionalAnalysis_AllTissues/ConditionalAnalysis_AllTissues.*.rds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Apply posterior calculations\n",
    "[posterior]\n",
    "parameter: mash_model = path(f\"{vhat_data:n}.mash_model.rds\")\n",
    "parameter: posterior_input = paths()\n",
    "parameter: posterior_vhat_files = paths()\n",
    "# eg, if data is saved in R list as data$strong, then\n",
    "# when you specify `--data-table-name strong` it will read the data as\n",
    "# readRDS('{_input:r}')$strong\n",
    "parameter: data_table_name = ''\n",
    "parameter: bhat_table_name = 'Bhat'\n",
    "parameter: shat_table_name = 'Shat'\n",
    "\n",
    "skip_if(len(posterior_input) == 0, msg = \"No posterior input data to compute on. Please specify it using --posterior-input.\")\n",
    "fail_if(len(posterior_vhat_files) > 1 and len(posterior_vhat_files) != len(posterior_input), msg = \"length of --posterior-input and --posterior-vhat-files do not agree.\")\n",
    "for p in posterior_input:\n",
    "    fail_if(not p.is_file(), msg = f'Cannot find posterior input file ``{p}``')\n",
    "\n",
    "depends: R_library(\"mashr\")\n",
    "input: posterior_input, group_by = 1\n",
    "output: f\"{_input:n}.posterior.rds\"\n",
    "    \n",
    "task: trunk_workers = 1, walltime = '20h', trunk_size = 1, mem = '20G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(mashr)\n",
    "    data = readRDS(${_input:r})${('$' + data_table_name) if data_table_name else ''}\n",
    "    vhat = readRDS(\"${vhat_data if len(posterior_vhat_files) == 0 else posterior_vhat_files[_index]}\")\n",
    "    mash_data = mash_set_data(data$${bhat_table_name}, Shat=data$${shat_table_name}, alpha=${1 if effect_model == 'EZ' else 0}, V=vhat)\n",
    "    saveRDS(mash_compute_posterior_matrices(readRDS(${mash_model:r}), mash_data, algorithm = \"${implementation}\"), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Posterior results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. The outcome of the `[posterior]` step should produce a number of serialized R objects `*.batch_*.posterior.rds` (can be loaded to R via `readRDS()`) -- I chopped data to batches to take advantage of computing in multiple cluster nodes. It should be self-explanary but please let me know otherwise.\n",
    "2. Other posterior related files are:\n",
    "    1. `*.batch_*.yaml`: gene-SNP pairs of interest, identified elsewhere (eg. fine-mapping analysis). \n",
    "    2. The corresponding univariate analysis summary statistics for gene-SNPs from `*.batch_*.yaml` are extracted and saved to `*.batch_*.rds`, creating input to the `[posterior]` step.\n",
    "    3. Note the `*.batch_*.stdout` file documents some SNPs found in fine-mapping results but not found in the original `fastqtl` output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.20.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
